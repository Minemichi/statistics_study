{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 第4章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4-2 尤度\n",
    "パラメタが決まった時に標本が得られる確率(密度)  \n",
    "↓  \n",
    "パラメタを持っている確率を出す式の出力みたいなもの\n",
    "\n",
    "## 4-3 尤度関数\n",
    "上記の\"式\"のこと\n",
    "\n",
    "$\n",
    "L(\\theta) = \\theta \\times (1-\\theta)\n",
    "$ \n",
    "　←表1裏1の出る尤度関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4-4 対数尤度\n",
    "尤度の対数  \n",
    "対数にすると計算が楽，微分とか．\n",
    "\n",
    "## 4-5 対数の性質\n",
    "略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4-6 最尤法\n",
    "尤度が最大になるパラメタを推定すること\n",
    "\n",
    "## 4-7 最尤推定量\n",
    "最尤法で推定されたパラメタ$\\hat{\\theta}$\n",
    "\n",
    "## 4-8 最大化対数尤度\n",
    "最尤推定量の対数尤度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4-9 正規分布に従うデータの尤度\n",
    "$ y \\sim N(\\mu , \\sigma ^2) $　　←平均$\\mu$, 分散$ \\sigma ^2$の正規分布  \n",
    "$ L = N(y_1 | \\mu , \\sigma ^2) \\times N(y_2 | \\mu , \\sigma ^2) $　　←サンプルサイズ2の尤度関数\n",
    "\n",
    "$L$を最大にする$\\mu$と$\\sigma ^2$を最尤法で求める"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4-10 局外パラメタ\n",
    "考えなくていいパラメタ\n",
    "\n",
    "$\\sigma ^2$は$\\mu$から求められる　→　$\\sigma ^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2$\n",
    "\n",
    "$\\sigma ^2$は局外パラメタとして$\\mu$だけ推定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4-11 正規線形モデルの尤度\n",
    "ビールの売り上げ$\\sim N(\\beta_0 + \\beta_1 \\times temperature, \\sigma^2)$\n",
    "\n",
    "正規分布$ N(\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi}\\sigma} exp\\{ - \\frac{(x - \\mu)^2}{2 \\sigma^2} \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4-12 最尤法の計算例\n",
    "微分したら最大値が出る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 4-13 最尤推定量の持つ性質\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 第5章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5-1 損失関数\n",
    "パラメタ推定の際の指標．これを小さくすることを目的とする．\n",
    "\n",
    "## 5-2 残差\n",
    "実際の値と推測値の差のこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5-3 残差の合計\n",
    "残差の合計が0になるパターンが複数あるので残差の合計を損失関数にはできない\n",
    "\n",
    "当てはまりがいいってなんだっけ…．\n",
    "\n",
    "## 5-4 残差平方和\n",
    "残差を平方(2乗)してから和をとる．二乗誤差とかの呼び方も．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5-5 最小二乗法\n",
    "残差平方和(=損失関数)を最小化するパラメタを採用する手法\n",
    "\n",
    "## 5-6 最小二乗法と最尤法の関係\n",
    "最尤法の式を変形すると同じになる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5-7 誤差関数\n",
    "機械学習分野の呼び方みたいなもの\n",
    "\n",
    "## 5-8 様々な損失関数\n",
    "正規線形モデルにおいては最小二乗法が使える  \n",
    "↓  \n",
    "正規分布以外の母集団分布の時には使えない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 第6章\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6-1 当てはめ精度・予測精度\n",
    "当てはめ精度　→　手持ちデータへの精度   \n",
    "予測精度　→　まだ見ぬデータへの汎用性\n",
    "\n",
    "## 6-2 過学習\n",
    "手持ちデータに特化しすぎてまだ見ぬデータへの汎用性が低い状態"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6-3 変数選択の意義\n",
    "説明変数が多すぎると過学習を起こしやすい　→　手持ちデータのすごく細かいとこまで考えてしまう感じ\n",
    "\n",
    "## 6-4 汎化誤差\n",
    "予測誤差のこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6-5 訓練データ・テストデータ\n",
    "訓練データ　→　手持ちデータのこと  \n",
    "テストデータ　→　まだ見ぬデータ(ということにするために手持ちデータから分割してパラメタ推定の時には使わないデータ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6-6 クロスバリデーション\n",
    "https://newtechnologylifestyle.net/機械学習、ディープラーニングでの学習データと/  \n",
    "ここの図が見やすい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6-7 赤池の情報量規準\n",
    "$AIC = -2 \\times (Max-log-likelihood - param_num)$　←小さい方がいい\n",
    "\n",
    "パラメタが多すぎるとスコアが出ない…過学習対策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6-8 相対エントロピー\n",
    "## 6-9 相対エントロピーの最小化と平均対数尤度\n",
    "相対エントロピーの最小化 = $\\int -g(y) \\log f(y)dy$  \n",
    "$ -g(y) \\log f(y)dy $の部分を平均対数尤度という．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6-10 平均対数尤度の持つバイアスとAIC\n",
    "平均対数尤度は何度も最尤法を解いた平均値→最大化対数尤度で代用\n",
    "\n",
    "式(4-27)より$ N(\\mu, \\sigma^2) $なら$ arg_\\mu max \\sum^n_{i=1} [ \\log (\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} - \\frac{(y_i - \\mu)^2}{2 \\sigma^2}) ]$\n",
    "\n",
    "$ N(\\beta_0 + \\beta_1 \\times temperature, \\sigma^2) $なら$ arg_{\\beta_0 + \\beta_1 \\times temperature} max \\sum^n_{i=1} [ \\log (\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} - \\frac{(y_i - (\\beta_0 + \\beta_1 \\times temperature))^2}{2 \\sigma^2}) ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "パラメタがいっぱいあると$ arg_{略} max \\sum^n_{i=1} [ \\log (\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} - \\frac{(y_i - (ax_1 + ax_2 + … + ax_n))^2}{2 \\sigma^2}) ]$\n",
    "\n",
    "パラメタが無限に多かったら最大尤度が出る…のでAICは最大化対数尤度を使う代わりにパラメタ数を引く"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6-13 検定の代わりとしての変数選択\n",
    "検定…データが正しいかを平均値から見る\n",
    "\n",
    "AIC…学習の精度を見る"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
